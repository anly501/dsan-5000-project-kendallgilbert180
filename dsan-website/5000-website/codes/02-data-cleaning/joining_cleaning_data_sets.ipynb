{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               Country       2000      2001      2002  \\\n",
      "1                             Australia  376896.50  377198.5  399905.8   \n",
      "2                               Belgium      55.25     55.25     55.25   \n",
      "3                                Canada   24661.75  24718.75   24918.5   \n",
      "4                                 Chile    8849.75   8849.75   8849.75   \n",
      "5                              Colombia   29421.75  29421.75  29421.75   \n",
      "..                                  ...        ...       ...       ...   \n",
      "128                   Wallis and Futuna       0.00         0         0   \n",
      "129                        OECD America  264910.80    264995    265537   \n",
      "130                   OECD Asia Oceania  682471.00  766921.8  789652.8   \n",
      "131  Areas beyond national jurisdiction       0.00        ..        ..   \n",
      "132                                 NaN        NaN       NaN       NaN   \n",
      "\n",
      "0        2003      2004       2005      2006      2007      2008  ...  \\\n",
      "1      399923  402051.8  406363.80  412437.8  417115.8  417560.3  ...   \n",
      "2       58.25     58.25     350.25    350.25    350.25    350.25  ...   \n",
      "3       28145   30039.5   32196.50     32534  32740.75  41618.75  ...   \n",
      "4     8868.75   10068.5   10177.00   10218.5  10219.75  10219.75  ...   \n",
      "5    29421.75  29421.75   60890.50   60906.5   60906.5   60910.5  ...   \n",
      "..        ...       ...        ...       ...       ...       ...  ...   \n",
      "128         0         0       0.00         0         0         0  ...   \n",
      "129    268851  281953.5  322805.30   1818255   1822393   1833613  ...   \n",
      "130  790187.3  792403.5  796863.50  803005.8   1924599   1925414  ...   \n",
      "131        ..        ..   22761.99        ..        ..        ..  ...   \n",
      "132       NaN       NaN        NaN       NaN       NaN       NaN  ...   \n",
      "\n",
      "0          2013        2014        2015        2016        2017        2018  \\\n",
      "1     830412.00   830600.80   830606.00   847506.80   855714.80  3030763.00   \n",
      "2       1248.25     1249.75     1249.75     1249.75     1249.75     1249.75   \n",
      "3      52431.00    52434.50    53968.75    56240.50   168553.00   179983.50   \n",
      "4     160674.50   160675.30   160896.30   461388.80   486509.30  1340514.00   \n",
      "5      63135.50    63829.00    63829.00    63832.75    94753.50    95429.75   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "128        0.00        0.00        0.00        0.00        0.00        0.00   \n",
      "129  2005840.00  2006540.00  2008303.00  2846838.00  3157774.00  4023892.00   \n",
      "130  2371645.00  2374724.00  2374849.00  2391879.00  2400109.00  4576173.00   \n",
      "131   535922.90   537198.60   537290.00   650146.10  1056335.00  1056617.00   \n",
      "132         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "0          2019        2020      2021      2022  \n",
      "1    3036163.00  3036163.00   3036163   3036163  \n",
      "2       1249.75     1312.50    1312.5    1312.5  \n",
      "3     508196.30   520922.00    520923    520923  \n",
      "4    1509188.00  1509192.00   1509216   1510300  \n",
      "5      95429.75    95429.75  95429.75  95429.75  \n",
      "..          ...         ...       ...       ...  \n",
      "128        0.00        0.00         0         0  \n",
      "129  4520824.00  4534215.00   4534240   4643005  \n",
      "130  4581647.00  4809421.00   4809421   4809421  \n",
      "131  1079974.00  1079974.00        ..        ..  \n",
      "132         NaN         NaN       NaN       NaN  \n",
      "\n",
      "[132 rows x 24 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Data into Python Environment\n",
    "\n",
    "#Importing / Cleaning Marine Pollution Act Data\n",
    "import pandas as pd\n",
    "\n",
    "#Importing in Data\n",
    "mpa = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/mpa.csv\")\n",
    "\n",
    "#Fixing Headers\n",
    "new_header = mpa.iloc[0] #grab the first row for the header\n",
    "mpa = mpa[1:] #take the data less the header row\n",
    "mpa.columns = new_header #set the header row as the df header\n",
    "mpa = mpa.rename(columns={'Year': 'Country', 2000.0: 2000, 2005.0: 2005, 2010.0: 2010, 2011.0: 2011, 2012.0: 2012, 2013.0: 2013, 2014.0: 2014, 2015.0: 2015,2016.0: 2016,2017.0: 2017,\n",
    "                          2018.0: 2018,2019.0: 2019,2020.0: 2020,})\n",
    "\n",
    "df_stripped = mpa.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "print(df_stripped)\n",
    "\n",
    "melted_df = pd.melt(mpa, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df['Country_Year'] = melted_df['Country'] + '_' + melted_df['Year'].astype(str)\n",
    "\n",
    "# Drop the original \"Country\" and \"Year\" columns\n",
    "melted_df = melted_df.drop(columns=['Country', 'Year'])\n",
    "\n",
    "melted_df_l = list(melted_df.Country_Year.value_counts())\n",
    "melted_df_l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Value     Country_Year\n",
       "0       485.1   Australia_2000\n",
       "1        15.5     Austria_2000\n",
       "2        5.65     Belgium_2000\n",
       "3      605.08      Canada_2000\n",
       "4     2124.58       Chile_2000\n",
       "...       ...              ...\n",
       "1129  1895.16    Thailand_2020\n",
       "1130  8788.29    Viet Nam_2020\n",
       "1131      NaN              NaN\n",
       "1132      NaN              NaN\n",
       "1133      NaN              NaN\n",
       "\n",
       "[1134 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "a_p = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/aqua_production.csv\")\n",
    "a_p.tail()\n",
    "new_header_2 = a_p.iloc[0] #grab the first row for the header\n",
    "a_p = a_p[1:] #take the data less the header row\n",
    "a_p.columns = new_header_2 #set the header row as the df header\n",
    "a_p = a_p.rename(columns={'Year': 'Country', 2000.0: 2000, 2001.0: 2001, 2002.0: 2002, 2003.0: 2003, 2004.0: 2004, 2005.0: 2005, 2006.0: 2006,2007.0: 2007,2008.0: 2008,\n",
    "                          2009.0: 2009,2010.0: 2010,2011.0: 2011,2005.0: 2005, 2010.0: 2010, 2011.0: 2011, 2012.0: 2012, 2013.0: 2013, 2014.0: 2014, 2015.0: 2015,2016.0: 2016,2017.0: 2017,\n",
    "                          2018.0: 2018,2019.0: 2019,2020.0: 2020,})\n",
    "\n",
    "df_stripped = a_p.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "melted_df_2 = pd.melt(a_p, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "melted_df_2.head\n",
    "\n",
    "melted_df_2.columns = melted_df_2.columns.str.strip()\n",
    "\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df_2['Country_Year'] = melted_df_2['Country'] + '_' + melted_df_2['Year'].astype(str)\n",
    "melted_df_2 = melted_df_2.drop(columns=['Country', 'Year'])\n",
    "\n",
    "melted_df_2.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007',\n",
       "       '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016',\n",
       "       '2017', '2018', '2019'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventions = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/ocean_related_inventions.csv\")\n",
    "new_header_3 = inventions.iloc[0] #grab the first row for the header\n",
    "inventions = inventions[1:] #take the data less the header row\n",
    "inventions.columns = new_header_3 #set the header row as the df header\n",
    "\n",
    "inventions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventions = inventions.rename(columns={'Year': 'Country'})\n",
    "\n",
    "df_stripped = inventions.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "melted_df_3 = pd.melt(inventions, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "melted_df_3.head\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df_3['Country_Year'] = melted_df_3['Country'] + '_' + melted_df_3['Year'].astype(str)\n",
    "melted_df_3 = melted_df_3.drop(columns=['Country', 'Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country',      2000,      2001,      2002,      2003,      2004,\n",
       "            2005,      2006,      2007,      2008,      2009,      2010,\n",
       "            2011,      2012,      2013,      2014,      2015,      2016,\n",
       "            2017,      2018,      2019,      2020,      2021,      2022],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/policy_instruments.csv\")\n",
    "new_header_4 = instruments.iloc[0] #grab the first row for the header\n",
    "instruments = instruments[1:] #take the data less the header row\n",
    "instruments.columns = new_header_4 #set the header row as the df header\n",
    "instruments.columns\n",
    "\n",
    "instruments = instruments.rename(columns={'Year': 'Country', 2000.0: 2000, 2001.0: 2001, 2002.0: 2002, 2003.0: 2003, 2004.0: 2004, 2005.0: 2005, 2006.0: 2006,2007.0: 2007,2008.0: 2008,\n",
    "                          2009.0: 2009,2010.0: 2010,2011.0: 2011,2005.0: 2005, 2010.0: 2010, 2011.0: 2011, 2012.0: 2012, 2013.0: 2013, 2014.0: 2014, 2015.0: 2015,2016.0: 2016,2017.0: 2017,\n",
    "                          2018.0: 2018,2019.0: 2019,2020.0: 2020, 2021.0:2021, '2022': 2022})\n",
    "\n",
    "instruments.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Value            Country_Year\n",
       "0      5.0          Australia_2000\n",
       "1      1.0            Austria_2000\n",
       "2      0.0            Belgium_2000\n",
       "3      8.0             Canada_2000\n",
       "4      4.0              Chile_2000\n",
       "...    ...                     ...\n",
       "3192    45       OECD America_2022\n",
       "3193    19  OECD Asia Oceania_2022\n",
       "3194   NaN                     NaN\n",
       "3195   NaN                     NaN\n",
       "3196   NaN                     NaN\n",
       "\n",
       "[3197 rows x 2 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = instruments.rename(columns={'Year': 'Country'})\n",
    "\n",
    "df_stripped = instruments.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "melted_df_4 = pd.melt(instruments, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "melted_df_4.head\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df_4['Country_Year'] = melted_df_4['Country'] + '_' + melted_df_4['Year'].astype(str)\n",
    "melted_df_4 = melted_df_4.drop(columns=['Country', 'Year'])\n",
    "\n",
    "instruments = instruments.rename(columns={'Value': 'Instrument Number'})\n",
    "\n",
    "\n",
    "melted_df_4.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country',      2000,      2001,      2002,      2003,      2004,\n",
       "            2005,      2006,      2007,      2008,      2009,      2010,\n",
       "            2011,      2012,      2013,      2014,      2015,      2016,\n",
       "            2017,      2018,      2019,      2020],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishing = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/fishing_exports.csv\")\n",
    "new_header_5 = fishing.iloc[0] #grab the first row for the header\n",
    "fishing = fishing[1:] #take the data less the header row\n",
    "fishing.columns = new_header_5 #set the header row as the df header\n",
    "fishing.columns\n",
    "\n",
    "fishing = fishing.rename(columns={'Year': 'Country', 2000.0: 2000, 2001.0: 2001, 2002.0: 2002, 2003.0: 2003, 2004.0: 2004, 2005.0: 2005, 2006.0: 2006,2007.0: 2007,2008.0: 2008,\n",
    "                          2009.0: 2009,2010.0: 2010,2011.0: 2011,2005.0: 2005, 2010.0: 2010, 2011.0: 2011, 2012.0: 2012, 2013.0: 2013, 2014.0: 2014, 2015.0: 2015,2016.0: 2016,2017.0: 2017,\n",
    "                          2018.0: 2018,2019.0: 2019,2020.0: 2020, 2021.0:2021, '2022': 2022})\n",
    "\n",
    "\n",
    "fishing.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Value           Country_Year\n",
       "0     1963.38         Australia_2000\n",
       "1       14.78           Austria_2000\n",
       "2      739.13           Belgium_2000\n",
       "3     4431.00            Canada_2000\n",
       "4     3118.12             Chile_2000\n",
       "...       ...                    ...\n",
       "1066   827.02       Philippines_2020\n",
       "1067  5191.08            Russia_2020\n",
       "1068  1387.52    Chinese Taipei_2020\n",
       "1069  4960.20          Thailand_2020\n",
       "1070  7958.36          Viet Nam_2020\n",
       "\n",
       "[1071 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishing = fishing.rename(columns={'Year': 'Country'})\n",
    "\n",
    "df_stripped = fishing.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "melted_df_5 = pd.melt(fishing, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "melted_df_5.head\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df_5['Country_Year'] = melted_df_5['Country'] + '_' + melted_df_5['Year'].astype(str)\n",
    "melted_df_5 = melted_df_5.drop(columns=['Country', 'Year'])\n",
    "\n",
    "melted_df_5.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = pd.read_csv(\"/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/data/00-raw-data/rd.csv\")\n",
    "new_header_6 = rd.iloc[0] #grab the first row for the header\n",
    "rd = rd[1:] #take the data less the header row\n",
    "rd.columns = new_header_6 #set the header row as the df header\n",
    "rd = rd.rename(columns={'Year': 'Country', '2000': 2000,'2001': 2001, '2002': 2002, '2003': 2003, '2004': 2004, '2005': 2005, '2006': 2006,\n",
    "                        '2007': 2007,'2008': 2008,'2009': 2009, '2010': 2010, '2011': 2011, '2012': 2012, '2013': 2013, '2014': 2014,\n",
    "                        '2015': 2015, '2016': 2016, '2017': 2017, '2018': 2018, '2019': 2019,'2020': 2020,'2021': 2021})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Value         Country_Year\n",
       "0       ..       Australia_2000\n",
       "1        0         Austria_2000\n",
       "2       ..         Belgium_2000\n",
       "3        0          Canada_2000\n",
       "4       ..  Czech Republic_2000\n",
       "..     ...                  ...\n",
       "721   0.96         TÃ¼rkiye_2021\n",
       "722  30.09  United Kingdom_2021\n",
       "723      0   United States_2021\n",
       "724     ..  European Union_2021\n",
       "725     ..          Brazil_2021\n",
       "\n",
       "[726 rows x 2 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd = rd.rename(columns={'Year': 'Country'})\n",
    "\n",
    "df_stripped = rd.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "melted_df_6 = pd.melt(rd, id_vars=['Country'], var_name='Year', value_name='Value')\n",
    "melted_df_6.head\n",
    "\n",
    "# Convert the \"Year\" column to a string and then combine \"Country\" and \"Year\" into \"Country_Year\"\n",
    "melted_df_6['Country_Year'] = melted_df_6['Country'] + '_' + melted_df_6['Year'].astype(str)\n",
    "melted_df_6 = melted_df_6.drop(columns=['Country', 'Year'])\n",
    "\n",
    "melted_df_6.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '..'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/codes/02-data-cleaning/joining_cleaning_data_sets.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/codes/02-data-cleaning/joining_cleaning_data_sets.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m merged \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(merged, melted_df_5, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCountry_Year\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minner\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/codes/02-data-cleaning/joining_cleaning_data_sets.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m merged \u001b[39m=\u001b[39m merged\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mValue_y\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mInstruments (number)\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mValue_x\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mInventions (number)\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mFishing Exports (Millions)\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kendallgilbert/dsan-5000-project-kendallgilbert180/dsan-website/5000-website/codes/02-data-cleaning/joining_cleaning_data_sets.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m correlation_matrix \u001b[39m=\u001b[39m merged[[\u001b[39m\"\u001b[39m\u001b[39mInstruments (number)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mInventions (number)\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mcorr()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:10054\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  10052\u001b[0m cols \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m  10053\u001b[0m idx \u001b[39m=\u001b[39m cols\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m> 10054\u001b[0m mat \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto_numpy(dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m, na_value\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m  10056\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpearson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m  10057\u001b[0m     correl \u001b[39m=\u001b[39m libalgos\u001b[39m.\u001b[39mnancorr(mat, minp\u001b[39m=\u001b[39mmin_periods)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:1838\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1837\u001b[0m     dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[0;32m-> 1838\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mas_array(dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, na_value\u001b[39m=\u001b[39mna_value)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype:\n\u001b[1;32m   1840\u001b[0m     result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(result, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1724\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(blk\u001b[39m.\u001b[39mget_values())\n\u001b[1;32m   1723\u001b[0m     \u001b[39mif\u001b[39;00m dtype:\n\u001b[0;32m-> 1724\u001b[0m         arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1726\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   1727\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '..'"
     ]
    }
   ],
   "source": [
    "merged = pd.merge(melted_df_2, melted_df, on='Country_Year', how='inner')\n",
    "merged = pd.merge(merged, melted_df_3, on='Country_Year', how='inner')\n",
    "merged = merged.rename(columns={'Value_x': 'Aqua Production (Millions)','Value_y': 'Marine Protected Area (sqkm)'})\n",
    "\n",
    "merged = pd.merge(merged, melted_df_4, on='Country_Year', how='inner')\n",
    "merged = pd.merge(merged, melted_df_5, on='Country_Year', how='inner')\n",
    "merged = merged.rename(columns={'Value_y': 'Instruments (number)','Value_x': 'Inventions (number)','Value': 'Fishing Exports (Millions)'})\n",
    "\n",
    "correlation_matrix = merged[[\"Instruments (number)\", \"Inventions (number)\"]].corr()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
